{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Truncate the original iEEG time series by taking all the seizure time points along with 5000 time points from before as well as after seizure.\n",
    "file_id     episode     start       dt  SzOnLoc         tend\ttend_sec\ttsz\t    tsz_sec\tdt_s\n",
    "111\t        15:42:30    15:40:46\t104\tL mesial temp\t43101\t215.505\t    34974\t174.87\t40.635\n",
    "112\t        17:04:35    17:02:51\t104\tL mesial temp\t38828\t194.14\t    26999\t134.995\t59.145\n",
    "113\t        19:42:49    19:41:06\t103\tR mesial temp\t35820\t179.1\t    25562\t127.81\t51.29\n",
    "'''\n",
    "\n",
    "def truncate_data(data, tstart, tend):\n",
    "    channels = data.columns[1:]\n",
    "    # Extract data for each channel as a NumPy array\n",
    "    channel_data = {}\n",
    "    for channel in channels:\n",
    "        channel_data[channel] = data[channel].to_numpy()\n",
    "\n",
    "    # Truncate data start-5000:end+5000\n",
    "    truncated_data = {}\n",
    "    for channel, data in channel_data.items():\n",
    "        truncated_data[channel] = data[tstart-5000:tend+5000]\n",
    "\n",
    "    # Save the truncated data to a new CSV file\n",
    "    truncated_data_df = pd.DataFrame(truncated_data)\n",
    "\n",
    "    return truncated_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fragmentize the truncated time series into 14 parts.\n",
    "Scheme: b/f seizure + seizure + a/f seizure = 2+10+2\n",
    "Length of the fragments, separately in the three regions, must be identical.\n",
    "'''\n",
    "\n",
    "def fragmentize_data(data, filename):\n",
    "    # add one more column to the data frame to store the fragment number\n",
    "    data['fragment'] = 0\n",
    "\n",
    "    # fragmentize the data\n",
    "    # divide the first 5000 points into 2 fragments\n",
    "    data.loc[0:2500, 'fragment'] = 1\n",
    "    data.loc[2501:5000, 'fragment'] = 2\n",
    "\n",
    "    # divide the last 5000 points into 2 fragments\n",
    "    data.loc[len(data)-5000:len(data)-2501, 'fragment'] = 13\n",
    "    data.loc[len(data)-2500:len(data), 'fragment'] = 14\n",
    "\n",
    "    # divide the seizure points into 10 fragments\n",
    "    seizure_points = data[data['fragment'] == 0].index\n",
    "    n = len(seizure_points)\n",
    "    for i in range(10):\n",
    "        data.loc[seizure_points[int(i*n/10):int((i+1)*n/10)], 'fragment'] = i+3\n",
    "        \n",
    "    # save the fragmentized data to a new CSV file\n",
    "    data.to_csv((filename.replace('.csv', '_fragmentized.csv')), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove reference channels and any channels that start with G, F, I. Column names must be only the channel names without any other characters in the string.\n",
    "Reference channels after cleaning: \"b'SEEG TLR03'\",\"b'SEEG TLR04'\"\n",
    "'''\n",
    "\n",
    "def clean_data(data, column_names):\n",
    "    # Remove reference channels\n",
    "    data = data.drop(columns=column_names)\n",
    "\n",
    "\n",
    "    # Rename the columns to remove the b' and ' characters\n",
    "    data.columns = [channel[2:-1] for channel in data.columns]\n",
    "    # also remove SEEG from the channel names\n",
    "    data.columns = [channel.replace('SEEG ', '') for channel in data.columns]\n",
    "    \n",
    "    # Remove channels that start with G, F, I\n",
    "    data = data[data.columns[~data.columns.str.startswith('G')]]\n",
    "    data = data[data.columns[~data.columns.str.startswith('F')]]\n",
    "    data = data[data.columns[~data.columns.str.startswith('I')]]\n",
    "    # also rearrange the columns\n",
    "    # in this order : TLL,TBAL,TBPL,TL,TR,TBPR,TBAR,TLR\n",
    "    # and each one within in alphabetical order, ie. TLL1, TLL2, TLL3, ...\n",
    "    # data = data.reindex(sorted(data.columns), axis=1)\n",
    "    tll_list = [channel for channel in data.columns if channel.startswith('TLL')]\n",
    "    tbal_list = [channel for channel in data.columns if channel.startswith('TBAL')]\n",
    "    tbpl_list = [channel for channel in data.columns if channel.startswith('TBPL')]\n",
    "    \n",
    "    tl_list = [channel for channel in data.columns if channel.startswith('TL')]\n",
    "    tl_list = [channel for channel in tl_list if not channel.startswith('TLR')]\n",
    "    tl_list = [channel for channel in tl_list if not channel.startswith('TLL')]\n",
    "\n",
    "    tr_list = [channel for channel in data.columns if channel.startswith('TR')]\n",
    "    tbpr_list = [channel for channel in data.columns if channel.startswith('TBPR')]\n",
    "    tbar_list = [channel for channel in data.columns if channel.startswith('TBAR')]\n",
    "    tlr_list = [channel for channel in data.columns if channel.startswith('TLR')]\n",
    "\n",
    "    # sort the lists\n",
    "    tll_list.sort()\n",
    "    tbal_list.sort()\n",
    "    tbpl_list.sort()\n",
    "    tl_list.sort()\n",
    "    tr_list.sort()\n",
    "    tbpr_list.sort()\n",
    "    tbar_list.sort()\n",
    "    tlr_list.sort()\n",
    "\n",
    "    data = data[tll_list + tbal_list + tbpl_list + tl_list + tr_list + tbpr_list + tbar_list + tlr_list]\n",
    "\n",
    "    # print the order of the columns\n",
    "    print(\"Order of the columns: \", data.columns)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of the columns:  Index(['TLL01', 'TLL02', 'TLL03', 'TLL04', 'TBAL3', 'TBAL4', 'TBPL1', 'TBPL2',\n",
      "       'TBPL3', 'TBPL4', 'TL01', 'TL02', 'TL03', 'TL04', 'TL05', 'TL06',\n",
      "       'TL07', 'TL08', 'TL09', 'TL10', 'TR01', 'TR02', 'TR03', 'TR04', 'TR05',\n",
      "       'TR06', 'TR07', 'TR08', 'TR09', 'TR10', 'TBPR1', 'TBPR2', 'TBPR3',\n",
      "       'TBPR4', 'TBAR1', 'TBAR2', 'TBAR3', 'TBAR4', 'TLR01', 'TLR02', 'TLR03',\n",
      "       'TLR04'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('029b0RMT.csv')\n",
    "truncated_data = truncate_data(data, 29095, 42573)\n",
    "cleaned_data = clean_data(truncated_data, [\"b\\'SEEG TBAL1\\'\",\"b\\'SEEG TBAL2\\'\"])\n",
    "data = fragmentize_data(cleaned_data, '029b0RMT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of the columns:  Index(['TLL01', 'TLL02', 'TLL03', 'TLL04', 'TBAL1', 'TBAL2', 'TBAL3', 'TBAL4',\n",
      "       'TBPL1', 'TBPL2', 'TBPL3', 'TBPL4', 'TL01', 'TL02', 'TL03', 'TL04',\n",
      "       'TL05', 'TL06', 'TL07', 'TL08', 'TL09', 'TL10', 'TR01', 'TR02', 'TR03',\n",
      "       'TR04', 'TR05', 'TR06', 'TR07', 'TR08', 'TR09', 'TR10', 'TBPR1',\n",
      "       'TBPR2', 'TBPR3', 'TBPR4', 'TBAR1', 'TBAR2', 'TBAR3', 'TBAR4', 'TLR01',\n",
      "       'TLR02', 'TLR03', 'TLR04'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('117h0RMT.csv')\n",
    "truncated_data = truncate_data(data, 35841, 47050)\n",
    "cleaned_data = clean_data(truncated_data, [\"b\\'SEEG FLL07\\'\",\"b\\'SEEG FLL08\\'\"])\n",
    "fragmentize_data(cleaned_data, '117h0RMT.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
